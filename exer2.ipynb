{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the findspark module \n",
    "import findspark\n",
    "\n",
    "# Initialize via the full spark path\n",
    "findspark.init(\"/usr/local/spark/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SparkSession module\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Gets an existing :class:`SparkSession` or, if there is no existing one, creates a\n",
    "# new one based on the options set in this builder.\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"Employees_Homework\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .getOrCreate()\n",
    "  \n",
    "# Main entry point for Spark functionality. A SparkContext represents the\n",
    "# connection to a Spark cluster, and can be used to create :class:`RDD` and\n",
    "# broadcast variables on that cluster.    \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    salary = int(fields[4])\n",
    "    name = str(fields[0])\n",
    "    return (name,salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile(\"./data/employees.csv\")\n",
    "# Return the first element\n",
    "header = data.first() \n",
    "# Parallelized collections are created by calling the parallelize method on an existing iterable or collection in your driver program. The elements of the collection are copied to form a distributed dataset that can be operated on in parallel.\n",
    "header = sc.parallelize([header])\n",
    "# Return without header\n",
    "lines = data.subtract(header)\n",
    "rdd = lines.map(parseLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of employees in each department {'Marketing': 74, 'Finance': 80, 'Product': 83, 'Engineering': 79, 'Business Development': 88, 'Legal': 68, 'Human Resources': 76, 'Sales': 72, 'Client Services': 85, 'Distribution': 60}\n",
      "Minimum Salary: ('Michael', 35013)\n",
      "Maximum Salary: ('Katherine', 149908)\n",
      "Mean Salary: 90444.1477124183\n"
     ]
    }
   ],
   "source": [
    "department = lines.map(lambda x:x.split(\",\")[7])\n",
    "result = dict(department.countByValue())\n",
    "print(\"Number of employees in each department\", result )\n",
    "\n",
    "min_salary = rdd.sortBy(lambda x: x[1]).first()\n",
    "# SortBy salary in descending order for maximum salary\n",
    "max_salary = rdd.sortBy(lambda x: -x[1]).first()\n",
    "\n",
    "salary = lines.map(lambda x: int(x.split(\",\")[4]))\n",
    "sum_salary = salary.sum()\n",
    "count = salary.count()\n",
    "mean_salary = sum_salary / count\n",
    "\n",
    "print(\"Minimum Salary:\", min_salary)\n",
    "print(\"Maximum Salary:\", max_salary)\n",
    "print(\"Mean Salary:\", mean_salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
