{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the findspark module \n",
    "import findspark\n",
    "\n",
    "# Initialize via the full spark path\n",
    "findspark.init(\"/usr/local/spark/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SparkSession module\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"Linear Regression Model\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .getOrCreate()\n",
    "   \n",
    "# Main entry point for Spark functionality.     \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a text file from HDFS\n",
    "rdd = sc.textFile('./data/cal_housing.data')\n",
    "\n",
    "# Read a text file from HDFS\n",
    "header = sc.textFile('./data/cal_housing.domain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a lambda function to split lines on commas\n",
    "rdd = rdd.map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the spark sql and sql.types, and sql.functions modules \n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Using the lambda function, for each input line of our rdd, we convert the line into a Row structure. Each row is composed of attributes from our data domain and their respective values.\n",
    "# Map the new RDD (after appyling lamda) to a DF using toDF()\n",
    "\n",
    "df = rdd.map (lambda line: Row(longitude=line[0], \n",
    "                              latitude=line[1], \n",
    "                              housingMedianAge=line[2],\n",
    "                              totalRooms=line[3],\n",
    "                              totalBedRooms=line[4],\n",
    "                              population=line[5], \n",
    "                              households=line[6],\n",
    "                              medianIncome=line[7],\n",
    "                              medianHouseValue=line[8])).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertColumn(df, names, newType):\n",
    "  for name in names: \n",
    "     df = df.withColumn(name, df[name].cast(newType))\n",
    "  return df \n",
    "\n",
    "# Assign column names to `columns`\n",
    "columns = ['households', 'housingMedianAge', 'latitude', 'longitude', 'medianHouseValue', 'medianIncome', 'population', 'totalBedRooms', 'totalRooms']\n",
    "\n",
    "# Convert the DataFrame columns to the floats\n",
    "df = convertColumn(df, columns, FloatType())\n",
    "\n",
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----------------------------+-----------------------------------+\n",
      "|medianIncomePerHousehold|medianHouseValuePerPopulation|medianHouseValuePerHousingMedianAge|\n",
      "+------------------------+-----------------------------+-----------------------------------+\n",
      "|                66.07302|                    1405.5901|                          11039.024|\n",
      "|                7.294728|                    149.31279|                          17071.428|\n",
      "|                41.00226|                      709.879|                           6771.154|\n",
      "|               25.767578|                    611.64874|                          6563.4614|\n",
      "|               14.850193|                     605.6637|                           6580.769|\n",
      "+------------------------+-----------------------------+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#I decided not to devide by 100.000, because we dont need scaling here and it looks more plausable \n",
    "df_1 = df.withColumn(\"medianIncomePerHousehold\", col(\"medianIncome\")*1000/col(\"households\")) \\\n",
    "   .withColumn(\"medianHouseValuePerPopulation\", col(\"medianHouseValue\")/col(\"population\")) \\\n",
    "   .withColumn(\"medianHouseValuePerHousingMedianAge\", col(\"medianHouseValue\")/col(\"housingMedianAge\"))\n",
    "\n",
    "columns = ['medianIncomePerHousehold', 'medianHouseValuePerPopulation', 'medianHouseValuePerHousingMedianAge']\n",
    "df_1 = convertColumn(df_1, columns, FloatType())\n",
    "# Returns the first row as a :class:`Row`.\n",
    "df_1.select('medianIncomePerHousehold','medianHouseValuePerPopulation', 'medianHouseValuePerHousingMedianAge').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"roomsPerHousehold\", col(\"totalRooms\")/col(\"households\")) \\\n",
    "   .withColumn(\"populationPerHousehold\", col(\"population\")/col(\"households\")) \\\n",
    "   .withColumn(\"bedroomsPerRoom\", col(\"totalBedRooms\")/col(\"totalRooms\"))\n",
    "# Returns the first row as a :class:`Row`.\n",
    "#df.select(\"roomsPerHousehold\",\"populationPerHousehold\",\"bedroomsPerRoom\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select( \"medianHouseValue\",\n",
    "                \"longitude\",\n",
    "                \"latitude\",\n",
    "                \"housingMedianAge\",\n",
    "                \"totalBedRooms\", \n",
    "                \"totalRooms\",\n",
    "                \"population\",\n",
    "                \"households\",\n",
    "                \"roomsPerHousehold\",\n",
    "                \"populationPerHousehold\",\n",
    "                \"bedroomsPerRoom\", \n",
    "                \"medianIncome\")\n",
    "#df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=452600.0, features=DenseVector([-122.23, 37.88, 41.0, 129.0, 880.0, 322.0, 126.0, 6.9841, 2.5556, 0.1466, 8.3252])),\n",
       " Row(label=358500.0, features=DenseVector([-122.22, 37.86, 21.0, 1106.0, 7099.0, 2401.0, 1138.0, 6.2381, 2.1098, 0.1558, 8.3014])),\n",
       " Row(label=352100.0, features=DenseVector([-122.24, 37.85, 52.0, 190.0, 1467.0, 496.0, 177.0, 8.2881, 2.8023, 0.1295, 7.2574])),\n",
       " Row(label=341300.0, features=DenseVector([-122.25, 37.85, 52.0, 235.0, 1274.0, 558.0, 219.0, 5.8174, 2.5479, 0.1845, 5.6431])),\n",
       " Row(label=342200.0, features=DenseVector([-122.25, 37.85, 52.0, 280.0, 1627.0, 565.0, 259.0, 6.2819, 2.1815, 0.1721, 3.8462]))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the DenseVector Module\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# Use a lambda function to create an RDD with the label and a dense vector (a value array) of features\n",
    "input_data = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "\n",
    "# Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
    "df_ml = spark.createDataFrame(input_data, [\"label\", \"features\"])\n",
    "\n",
    "# Prints the first row to the console.\n",
    "df_ml.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|   label|            features|\n",
      "+--------+--------------------+\n",
      "|452600.0|[-61.007270679927...|\n",
      "+--------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the StandardScaler Module\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Standardizes features by removing the mean and scaling to unit variance using column summary\n",
    "# statistics on the samples in the training set.\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "\n",
    "# Fits a model to the input dataset with optional parameters.\n",
    "scaler = standardScaler.fit(df_ml)\n",
    "\n",
    "# Transforms the input dataset with optional parameters.\n",
    "scaled_df = scaler.transform(df_ml)\n",
    "\n",
    "# Using scaled values or non scaled values doesn't have an effect here.\n",
    "scaled_df = scaled_df.select(\"label\",\"features_scaled\")\n",
    "scaled_df = scaled_df.withColumn(\"features\",col(\"features_scaled\"))\n",
    "scaled_df = scaled_df.select(\"label\",\"features\")\n",
    "\n",
    "# Prints the first row to the console.\n",
    "scaled_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly splits this :class:`DataFrame` with the provided weights.\n",
    "train_data, test_data = scaled_df.randomSplit([.8,.2],seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=14999.0, features=DenseVector([-61.4764, 18.8721, 2.8604, 0.0665, 0.0449, 0.0159, 0.0209, 4.9511, 0.2166, 4.9241, 0.2821])),\n",
       " Row(label=14999.0, features=DenseVector([-58.8261, 16.0303, 4.1317, 0.6338, 0.3681, 0.5545, 0.5885, 1.4425, 0.2687, 5.7305, 2.2072])),\n",
       " Row(label=14999.0, features=DenseVector([-58.4069, 17.0416, 1.5097, 0.5674, 0.2837, 0.4327, 0.4289, 1.5255, 0.2877, 6.6543, 1.1054])),\n",
       " Row(label=22500.0, features=DenseVector([-61.0522, 17.7579, 2.6221, 0.1733, 0.1357, 0.1907, 0.1648, 1.899, 0.3301, 4.2504, 1.408])),\n",
       " Row(label=22500.0, features=DenseVector([-60.5381, 17.7673, 4.1317, 0.1875, 0.049, 0.1475, 0.1386, 0.816, 0.3034, 12.7245, 0.4167]))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LinearRegression Module\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(labelCol=\"label\", maxIter=10000, regParam=0, elasticNetParam=1)\n",
    "\n",
    "# Fits a model to the input dataset with optional parameters.\n",
    "linearModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform  is used to perform value predictions using the trained model\n",
    "predicted = linearModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+\n",
      "|  label|            features|        prediction|\n",
      "+-------+--------------------+------------------+\n",
      "|14999.0|[-61.261818434728...| 81395.28159154393|\n",
      "|17500.0|[-59.060707297721...| 195855.7107780762|\n",
      "|33200.0|[-59.385135162751...| 93288.21803263761|\n",
      "|34600.0|[-60.328469051178...|-75413.89191184286|\n",
      "|35000.0|[-58.581553878836...|29673.063382339198|\n",
      "|36600.0|[-59.390123607217...|63185.855945119634|\n",
      "|36700.0|[-61.815840408308...| 43623.47798000416|\n",
      "|39400.0|[-60.662877613113...| 29124.33374533709|\n",
      "|40900.0|[-59.459999909473...|  74330.6296063033|\n",
      "|41300.0|[-61.501397048157...|158324.32299417863|\n",
      "|42100.0|[-59.674621068680...| 99524.94548265962|\n",
      "|42500.0|[-60.238627547139...|119665.66694469936|\n",
      "|42500.0|[-59.499930313040...| 72223.08001787867|\n",
      "|42500.0|[-59.395115859656...| 85448.20409646537|\n",
      "|42600.0|[-59.390123607217...| 54742.86744293338|\n",
      "|43000.0|[-60.553070907291...| 80267.83391900081|\n",
      "|43300.0|[-59.784427774503...| 64571.75558787072|\n",
      "|43300.0|[-57.653192939753...| 63597.33554791799|\n",
      "|43700.0|[-59.529876211729...| 76481.14543676935|\n",
      "|44100.0|[-59.789420026942...| 65912.20831697481|\n",
      "+-------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted.select(\"label\",\"features\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(81395.28159154393, 14999.0),\n",
       " (195855.7107780762, 17500.0),\n",
       " (93288.21803263761, 33200.0),\n",
       " (-75413.89191184286, 34600.0),\n",
       " (29673.063382339198, 35000.0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Projects a set of expressions and returns a new :class:`DataFrame`.\n",
    "predictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "\n",
    "# Projects a set of expressions and returns a new :class:`DataFrame`.\n",
    "labels = predicted.select(\"label\").rdd.map(lambda x: x[0])\n",
    "\n",
    "# Zips this RDD with another one, returning key-value pairs with the\n",
    "# first element in each RDD second element in each RDD, etc.\n",
    "predictionAndLabel = predictions.zip(labels).collect()\n",
    "\n",
    "# Built-in mutable sequence.\n",
    "# Print out first 5 instances of `predictionAndLabel` \n",
    "predictionAndLabel[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-82691.3385, -87375.313, 14365.4742, 10394.8402, 1688.6633, -46549.7439, 39245.7148, 7001.0789, -331.5516, 16960.1748, 80932.8868])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model coefficients.\n",
    "linearModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3554020.494834979"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model intercept.\n",
    "linearModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68245.64450390877"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns the root mean squared error, which is defined as the\n",
    "# square root of the mean squared error.\n",
    "linearModel.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6502674985280331"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns R^2, the coefficient of determination. More about this here: https://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/statistics/regression-and-correlation/coefficient-of-determination-r-squared.html\n",
    "linearModel.summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
