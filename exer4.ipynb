{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the findspark module \n",
    "import findspark\n",
    "\n",
    "# Initialize via the full spark path\n",
    "findspark.init(\"/usr/local/spark/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SparkSession and SQLContext modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"NLP Homework\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .getOrCreate()\n",
    "   \n",
    "# Main entry point for Spark functionality. \n",
    "sc = spark.sparkContext\n",
    "\n",
    "# The entry point for working with structured data (rows and columns) in Spark\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.8/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.8/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from nltk) (4.51.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk) (0.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install nltk --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langid in /opt/conda/lib/python3.8/site-packages (1.1.6)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from langid) (1.19.4)\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install langid --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download averaged_perceptron_tagger from the nltk\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download en averaged_perceptron_tagger from the nltk\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download stopwords from the nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download wordnet from the nltk\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the udf, StringType, and pp modules\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import preproc as pp\n",
    "import csv\n",
    "\n",
    "# Invoke the user-defined function `pp.check_lang` within the Spark UDF.\n",
    "# Refer to https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html\n",
    "# `pp.check_lang` is used to classify the language of our input text\n",
    "check_lang_udf = udf(pp.check_lang, StringType())\n",
    "\n",
    "# Invoke the user-defined function `pp.remove_stops` within the Spark UDF.\n",
    "# Stop words usually refer to the most common words in a language, there is no single universal list of stop words used\n",
    "# by all natural language processing tools.\n",
    "# Reduces Dimensionality\n",
    "# removes stop words of a single Tweets (cleaned_str/row/document)\n",
    "remove_stops_udf = udf(pp.remove_stops, StringType())\n",
    "\n",
    "# Invoke the user-defined function `pp.remove_features` within the Spark UDF.\n",
    "# catch-all to remove other 'words' that I felt didn't add a lot of value\n",
    "# Reduces Dimensionality, gets rid of a lot of unique urls\n",
    "remove_features_udf = udf(pp.remove_features, StringType())\n",
    "\n",
    "# Invoke the user-defined function `pp.tag_and_remove` within the Spark UDF.\n",
    "# Process of classifying words into their parts of speech and labeling them accordingly is known as part-of-speech\n",
    "# tagging, POS-tagging, or simply tagging. Parts of speech are also known as word classes or lexical categories. The\n",
    "# collection of tags used for a particular task is known as a tagset. Our emphasis in this chapter is on exploiting\n",
    "# tags, and tagging text automatically.\n",
    "# http://www.nltk.org/book/ch05.html\n",
    "tag_and_remove_udf = udf(pp.tag_and_remove, StringType())\n",
    "\n",
    "# Invoke the user-defined function `pp.lemmatize` within the Spark UDF.\n",
    "# Tweets are going to use different forms of a word, such as organize, organizes, and\n",
    "# organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy,\n",
    "# democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these\n",
    "# words to return documents that contain another word in the set.\n",
    "# Reduces Dimensionality and boosts numerical measures like TFIDF\n",
    "# http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    "# lemmatization of a single Tweets (cleaned_str/row/document)\n",
    "lemmatize_udf = udf(pp.lemmatize, StringType())\n",
    "\n",
    "# Invoke the user-defined function `pp.check_blanks` within the Spark UDF.\n",
    "# check to see if a row only contains whitespace\n",
    "check_blanks_udf = udf(pp.check_blanks, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['8476', 'You Can Smell Hillaryâ€™s Fear', '1'],\n",
       " ['10294',\n",
       "  'Watch The Exact Moment Paul Ryan Committed Political Suicide At A Trump Rally (VIDEO)',\n",
       "  '1'],\n",
       " ['3608', 'Kerry to go to Paris in gesture of sympathy', '0'],\n",
       " ['10142',\n",
       "  \"Bernie supporters on Twitter erupt in anger against the DNC: 'We tried to warn you!'\",\n",
       "  '1'],\n",
       " ['875', 'The Battle of New York: Why This Primary Matters', '0']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read a text file from HDFS\n",
    "data_rdd = sc.textFile(\"./data/fake_or_real_news.csv\")\n",
    "\n",
    "# Use a lambda function to split lines on tabs\n",
    "parts_rdd = data_rdd.map(lambda line: next(csv.reader([line], delimiter=',')))\n",
    "parts_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-----+\n",
      "|   id|            headline|label|\n",
      "+-----+--------------------+-----+\n",
      "| 8476|You Can Smell Hil...|    1|\n",
      "|10294|Watch The Exact M...|    1|\n",
      "+-----+--------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"id\", \"headline\", \"label\"]\n",
    "df = parts_rdd.map(lambda c: (int(c[0]), c[1], int(c[2]))).toDF(columns)\n",
    "df.show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Appling preprocessing functions from the lab2\n",
    "df = (df.withColumn(\"headline\", remove_stops_udf(col(\"headline\")))        # Remove stopwords\n",
    "       .withColumn(\"headline\", remove_features_udf(col(\"headline\")))     # Remove unnecessary features\n",
    "       .withColumn(\"headline\", tag_and_remove_udf(col(\"headline\")))      # POS tagging and filtering\n",
    "       .withColumn(\"headline\", lemmatize_udf(col(\"headline\")))           # Lemmatization\n",
    "       .withColumn(\"is_blank\", check_blanks_udf(col(\"headline\")))        # Check for blank rows\n",
    "       .filter(col(\"is_blank\") != \"true\")                                # Filter out blank headlines\n",
    ")\n",
    "\n",
    "final_df = df.select(\"headline\", \"label\")\n",
    "dedup_df = final_df.dropDuplicates([\"headline\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly splits this :class:`DataFrame` with the provided weights.\n",
    "splits = dedup_df.randomSplit([0.6, 0.4])\n",
    "training_df = splits[0]\n",
    "test_df= splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            headline|label|\n",
      "+--------------------+-----+\n",
      "|buffett run raise...|    0|\n",
      "|hillary melt wein...|    1|\n",
      "|assange donald tr...|    1|\n",
      "|be long overdue p...|    1|\n",
      "|historic climate ...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|            headline|label|\n",
      "+--------------------+-----+\n",
      "|          tehran usa|    1|\n",
      "|funny republican ...|    0|\n",
      "|putin congratulat...|    1|\n",
      "|justice clarence ...|    1|\n",
      "|latest patriot ac...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take the first num elements of the RDD.\n",
    "training_df.show(5)\n",
    "test_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|            headline|label|               words|        raw_features|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|buffett run raise...|    0|[buffett, run, ra...|(1000,[253,266,52...|(1000,[253,266,52...|[-89.906244871585...|[0.99998739403789...|       0.0|\n",
      "|hillary melt wein...|    1|[hillary, melt, w...|(1000,[214,314,38...|(1000,[214,314,38...|[-177.40078286069...|[1.91374778094053...|       1.0|\n",
      "|assange donald tr...|    1|[assange, donald,...|(1000,[70,115,366...|(1000,[70,115,366...|[-219.05588645695...|[3.43553588986293...|       1.0|\n",
      "|be long overdue p...|    1|[be, long, overdu...|(1000,[638,844,85...|(1000,[638,844,85...|[-120.13540184204...|[2.53078145030877...|       1.0|\n",
      "|historic climate ...|    0|[historic, climat...|(1000,[14,169,321...|(1000,[14,169,321...|[-191.28400010439...|[0.99999999999994...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "# tokenizer converts the input string to lowercase and then splits it by white spaces.\n",
    "tokenizer = Tokenizer(inputCol=\"headline\", outputCol=\"words\")\n",
    "\n",
    "# Converting the words into term frequency vectors\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"raw_features\", numFeatures=1000)\n",
    "\n",
    "# Computing the inverse document frequency (IDF) to downweight common words\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "#Introducing Naive Bayes\n",
    "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Pipeline to chain these steps\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, nb])\n",
    "\n",
    "# Fit and transform the training data to extract features\n",
    "model = pipeline.fit(training_df)\n",
    "featurized_df = model.transform(training_df)\n",
    "featurized_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Parameter grid for cross-validation\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(nb.smoothing, [0.0, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Cross-validation\n",
    "cv = CrossValidator(estimator=pipeline,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\"),\n",
    "                    numFolds=3,\n",
    "                    seed=42)  # Set seed for cross-validation reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|            headline|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|          tehran usa|    1|       1.0|\n",
      "|funny republican ...|    0|       1.0|\n",
      "|putin congratulat...|    1|       1.0|\n",
      "|justice clarence ...|    1|       0.0|\n",
      "|latest patriot ac...|    0|       1.0|\n",
      "|donald weak deleg...|    0|       0.0|\n",
      "|effect substance ...|    1|       0.0|\n",
      "|dems sue gop trum...|    1|       1.0|\n",
      "|don blame immigra...|    0|       0.0|\n",
      "|john kerry isi re...|    0|       0.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Accuracy = 0.4956\n",
      "F1-Score = 0.4972\n"
     ]
    }
   ],
   "source": [
    "# Fiting the model using cross-validation on the training data\n",
    "cvModel = cv.fit(training_df)\n",
    "\n",
    "# Making predictions on the test set\n",
    "predictions = cvModel.transform(test_df)\n",
    "predictions.select(\"headline\", \"label\", \"prediction\").show(10)\n",
    "\n",
    "# Evaluating the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "# Accuracy evaluation\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "print(f\"Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "# F1-score evaluation\n",
    "f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "print(f\"F1-Score = {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
